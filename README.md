# ğŸ¶ DOGE Data Challenge ğŸš€

ğŸ›ï¸ A data-driven look into U.S. federal regulations using the eCFR API â€” exploring word counts, trends over time, and custom metrics like regulatory density to inform smarter de-regulation strategies

ğŸ“œ Built to support data transparency and government-wide efficiency efforts by analyzing the Code of Federal Regulations (CFR), unleashing prosperity through de-regulation

âœ’ï¸ Hamilton had his pen â€”â€” I have my keyboard... Using data to untangle the regulatory state, one agency at a time

---

## ğŸ¯ Project Purpose

This technical assessment explores how to better understand and visualize the scale and complexity of U.S. federal regulations:

- The **eCFR** contains over **200,000 pages** of regulatory text across ~150 agencies
- The data is publicly accessible through an [official API](https://www.ecfr.gov/reader-aids/ecfr-developer-resources/rest-api-interactive-documentation)
- The goal: **build a tool to parse and analyze regulation data** for actionable insights

---

## Overview
The DOGE Data Challenge processes regulation XML snapshots to map agencies, extract text, and produce analytical reports. Key features include:

- **Dynamic Configuration:** Paths and settings are managed via a .env file, with defaults set by bootstrap.py
- **Modular Utilities:** Helper functions (i.e., path management, text processing) are packaged with **Poetry** for reusability
- **Notebook Pipeline:** Several **Jupyter** notebooks handle data ingestion, processing, analysis, and visualization
- **Testing:** Unit tests ensure reliability of core utilities

---

## âš¡ Quick Setup
### 1ï¸âƒ£ Install Poetry:
```bash
pip install poetry
```
Manual Path Setup: If ~/.local/bin isnâ€™t in your PATH, you may need to add it manually (e.g., export PATH="$HOME/.local/bin:$PATH")

### 2ï¸âƒ£ Clone the repository:
```bash
git clone https://github.com/bkaewell/doge-data-challenge.git
cd doge-data-challenge
```

### 3ï¸âƒ£ Install dependencies:
```bash
poetry install
```
Verify pyproject.toml is up to date with all dependencies (pandas, matplotlib, etc.);
poetry add python-dotenv jupyter
This doge-data-challenge project uses Poetry to manage dependencies and package doge_data_challenge/helpers/

### 4ï¸âƒ£ Bootstrap project:
```bash
poetry run python bootstrap.py
```
This creates a .env file with default values if none exists.

### 5ï¸âƒ£ Run notebooks:
```bash
poetry run jupyter-notebook
```

---

## Usage
- Configure .env (copy .env.example and edit) to set SNAPSHOT_DATE, ARCHIVE_DIR, etc.
- Run notebooks in order (01_agency_mapping_and_flattening.ipynb to 04_visualization_and_reporting.ipynb).
- Each notebook uses init_notebook() to load paths:
```python
from doge_data_challenge.helpers import init_notebook
paths = init_notebook()
```

---

## ğŸ”¢ Word Count Methods

The `WORDCOUNT_METHOD` defined in your `.env` controls how regulation text is parsed and counted. This ensures transparency and consistency across analyses â€” especially when comparing different agencies or dates.

| Method   | Description                                                                |
|----------|----------------------------------------------------------------------------|
| `split`  | Simple `text.split()` based on whitespace â€” fast but may over/under count  |
| `regex`  | Uses `\b\w+\b` to match real words â€” closer to Google Docs word count      |
| `legal`  | Placeholder for stricter rules (i.e. exclude citations, headers, numbers)  |
| `nlp`    | Placeholder for future spaCy/NLTK-style tokenization                       |

---

## ğŸ“‚ Repository Overview  
```
doge-data-challenge/
â”œâ”€â”€ README.md               # Documentation (this file)
â”œâ”€â”€ bootstrap.py            # Sets up .env with default values
â”œâ”€â”€ doge_data_challenge/
â”‚   â”œâ”€â”€ __init__.py         # Marks doge_data_challenge as a Poetry Python package
â”‚   â””â”€â”€ helpers/            # Reusable utility functions
â”‚       â”œâ”€â”€ __init__.py                # Exposes helper functions for import
â”‚       â”œâ”€â”€ env_paths.py               # Loads .env and defines project paths
â”‚       â”œâ”€â”€ init_notebook.py           # Initializes notebooks with sys.path and paths
â”‚       â”œâ”€â”€ print_helpers.py           # Formats and prints directory status messages
â”‚       â”œâ”€â”€ trim_notebook_outputs.py   # Limits notebook output size for Git
â”‚       â””â”€â”€ wordcount.py               # Implements word counting strategies
â”œâ”€â”€ notebooks/                                  # Data pipeline notebooks
â”‚   â”œâ”€â”€ 01_agency_scraper.ipynb                 # Scrapes, maps, and flattens agency JSON to a dataframe
â”‚   â”œâ”€â”€ 02_data_download_and_storage.ipynb      # Downloads and caches XMLs
â”‚   â”œâ”€â”€ 03_text_extraction_and_analysis.ipynb   # Extracts and analyzes text
â”‚   â””â”€â”€ 04_visualization_and_reporting.ipynb    # Generates metrics and charts
â”œâ”€â”€ tests/                                      # Unit tests for reliability
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_env_paths.py            # Tests path loading and directory creation
â”œâ”€â”€ .env                             # Configuration file
â”œâ”€â”€ .gitignore                       # Ignores .env, data, checkpoints, and cache files
â”œâ”€â”€ poetry.lock                      # Locks dependency versions (Git-ignored)
â”œâ”€â”€ pyproject.toml                   # Poetry configuration and dependencies
â”‚
â”‚                                    # Below is Git-ignored to keep repo lightweight
â”‚                                    ###############################################
â”œâ”€â”€ {AGENCY_METADATA_DIR}/           # Stores metadata
â”‚   â””â”€â”€ {SNAPSHOT_DATE}/             
â”‚       â”œâ”€â”€ agencies.json            # Top-level JSON from API
â”‚       â””â”€â”€ flattened_agencies.csv   # Output from 01_agency_scraper
â”‚   ...
â””â”€â”€ {REGULATION_TEXT_DIR}/           # Regulation XMLs from API
â”‚   â””â”€â”€ {SNAPSHOT_DATE}/             
â”‚   ...                              # Output from 02_data_download_and_storage 
```  

---

## ğŸ“š Additional Documentation  

TBD

---

## ğŸ¤ Contributing & Contact    

**ğŸ¯ Looking to contribute?** Open an issue or fork the repo!  
**ğŸ‘¨â€ğŸ’» Author:** [Brian Kaewell](https://github.com/bkaewell)  
**ğŸ“§ Contact:** Please open an issue [here](https://github.com/bkaewell/doge-data-challenge/issues)

---

## **BACKUP**

## ğŸ“Œ Key Deliverables

- Download and parse regulation text from the eCFR API
- Compute word counts, track changes over time (i.e. 2020 â†’ 2025), and generate SHA-256 checksums per agency
- Normalize nested agency structures (including children) for accurate aggregation
- Introduce a custom metric: **regulatory density** = words per CFR reference
- Visualize agency sizes and regulation growth
- Build a modular pipeline for future extension (i.e. NLP-based analysis)

## âš¡ Quick Setup
### 1ï¸âƒ£ Clone the Repo  
```bash
git clone https://github.com/bkaewell/doge-data-challenge.git
cd doge-data-challenge
```

### 2ï¸âƒ£ Bootstrap Your Environment  
```bash
python bootstrap.py
```

This will:
- âœ… Create a `.env` file if it doesn't exist
- âœ… Set the default `SNAPSHOT_DATE` to today
- âœ… Set the default `WORDCOUNT_METHOD` to `regex`
- âœ… Create the necessary data folders under `data/` and `archive/`

---

Context: The project involves scraping agency metadata and regulation texts, organized by SNAPSHOT_DATE. The .env file defines AGENCY_METADATA_DIR and REGULATION_TEXT_DIR, and users may extend it for snapshot-specific configurations




## Configuration
All configuration lives in .env. You can manually set a specific date for analysis:
```ini
SNAPSHOT_DATE=2025-03-27
WORDCOUNT_METHOD=regex  # Options: split, regex, legal, nlp

ARCHIVE_DIR=archive
DATA_DIR=data
```
  > ğŸ”¥ğŸ”¥ `regex` balances speed and accuracy with NLP-style tokenization

---